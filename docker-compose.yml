services:
  postgres:
    image: postgres:16
    container_name: flight_postgres
    env_file: .env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports: ["5432:5432"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 20
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./db/00_warehouse.sql:/docker-entrypoint-initdb.d/00_warehouse.sql:ro
      - ./db/01_views.sql:/docker-entrypoint-initdb.d/01_views.sql:ro

  kafka:
    image: bitnami/kafka:3.7
    container_name: flight_kafka
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9094,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9094
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR=1
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - ALLOW_PLAINTEXT_LISTENER=yes
    ports:
      - "${HOST_KAFKA_PORT:-9094}:9094"
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9094 --list >/dev/null 2>&1 || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 30
      start_period: 20s

  producer:
    build: ./apps/producer
    env_file: .env
    depends_on: 
      kafka: {condition: service_healthy}
    restart: unless-stopped

# ✂️ remove spark-master, spark-worker-1, and old spark-app

  spark:
    build: ./apps/spark_app
    container_name: flight_spark
    depends_on: 
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file: .env
    environment:
      # Make sure these exist in your .env (or inline here)
      - KAFKA_BOOTSTRAP=kafka:9094          # ← your KRaft broker inside Compose
      - KAFKA_TOPIC=${KAFKA_TOPIC}          # flights_live
      - PGHOST=${PGHOST}                    # flight_postgres
      - PGPORT=${PGPORT}                    # 5432
      - PGDATABASE=${PGDATABASE}            # flight_pipeline
      - PGUSER=${PGUSER}                    # flight_user
      - PGPASSWORD=${PGPASSWORD}            # Password123
      - CHECKPOINT_DIR=${CHECKPOINT_DIR:-/tmp/chk/flights_stream}
    # Use ARRAY form so args are not split by the shell
    command:
      - /opt/bitnami/spark/bin/spark-submit
      - --master
      - local[*]
      - --conf
      - spark.jars.ivy=/opt/bitnami/spark/app/ivy
      - --packages
      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6,org.postgresql:postgresql:42.7.3   # <— 3.5.6
      - /opt/bitnami/spark/app/flight_stream.py
    volumes:
      - spark_ivy:/opt/bitnami/spark/app/ivy 
    restart: unless-stopped

  loader:
    build: ./apps/loader
    container_name: loader
    depends_on:
      postgres:
        condition: service_healthy
    env_file: .env
    command: ["python", "load_warehouse.py", "--interval-seconds", "60"]
    restart: unless-stopped
  
  sheets-sink:
    build: ./apps/sheets_sink
    container_name: flight_gsheet
    depends_on: 
      postgres:
        condition: service_healthy
    env_file: .env
    environment:
      - GOOGLE_APPLICATION_CREDENTIALS=/secrets/gsa-keys.json   # <-- fix
      - GSHEET_ID=${GSHEET_ID}
      - GSHEET_TAB=${GSHEET_TAB}
      - PGHOST=${PGHOST}
      - PGPORT=${PGPORT}
      - PGDATABASE=${PGDATABASE}
      - PGUSER=${PGUSER}
      - PGPASSWORD=${PGPASSWORD}
      - GSHEET_BATCH_SIZE=${GSHEET_BATCH_SIZE:-300}
      - GSHEET_INTERVAL_SEC=${GSHEET_INTERVAL_SEC:-30}
    volumes:
      -  "${HOME}/gsa/gsa-keys.json:/secrets/gsa-keys.json:ro"
    restart: unless-stopped

volumes:
  pgdata:
  spark_ivy: